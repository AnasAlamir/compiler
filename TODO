Scanner (Lexical Analyzer) - To-Do List:
  1. Set Up Input Handling:
    [x] Read source code file line by line
    [x] Track and store the current line number
    [] Support and process include(F_name.txt); directives:
      [] Handle nested includes
      [] Ignore include if file not found

  2. Preprocess Input:
    [x] Remove and ignore:
      [x] Single-line comments: starts with /^
      [x] Multi-line comments: between /@ and @/
      [x] Whitespace and tabs (except for token separation)

  3. Token Recognition Logic:
    [x] Create a dictionary of keywords and their return tokens (from Table 1)
    Define token categories and regex patterns for:
      [x] Keywords (e.g., IfTrue, NOReturn, etc.)
      [x] Identifiers (start with a letter or _, followed by letters/digits)
      [x] Constants (integers)
      [x] Operators: arithmetic, relational, assignment
      [x] Braces and punctuation: {}, (), ;
      [] Strings and characters ("...", 'c')
      [] Access operator: ->
    Special cases:
      [x] Error on invalid identifiers (e.g., starting with a digit)

  4. Tokenization Process:
    Loop through each line:
      [x] Split by space/tab to extract tokens
      [x] Match each token using the token recognition logic
      For valid tokens:
        [x] Print line number, token text, and token type
      For invalid tokens:
        [x] Print line number, error message, and token text
        [x] Increment error count

  5. Error Handling:
    [x] Count and display total number of errors after scanning

  6. Output Format:
    [x] Match exactly the required output format:
      Line : X Token Text: Y Token Type: Z
      Line : X Error in Token Text: Y Token Type: Invalid Identifier
      Total NO of errors: N

  7. Testing:
    [x] Test using the provided sample input and output
    Create additional test cases to validate:
      [x] Comments
      [] Includes
      [] All token types
      [] Invalid identifiers

